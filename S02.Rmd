---
title: "S02"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---



```{r, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(imputeTS)
library(ggplot2)
library(forecast)
library(tseries)
library(gridExtra)
library(kableExtra)
library(scales)
library(openxlsx)
library(urca)
library(purrr)
```

# Load Data
```{r}
data <- read.csv("Data Set for Class.csv", stringsAsFactors = FALSE)
```

# Data Cleaning and Preprocessing
```{r}
S02 <- data %>% 
  mutate(date = as.Date(SeriesInd, origin = '1899-12-30')) %>%
  filter(SeriesInd < 43022 & category == 'S02') %>% 
  select(SeriesInd, date, category, Var03, Var02)

```
# Handle missing data
```{r}
S02$Var03 <- na_interpolation(S02$Var03)
S02$Var02 <- na_interpolation(S02$Var02)

glimpse(S02)
```
# Visualize Data

```{r}



ggplot(S02, aes(x = Var03)) + 
  geom_histogram(bins = 50, color = 'black', fill = 'orange') + 
  ggtitle("  S02-Var03 Histogram Distribution")

ggplot(S02, aes(x = Var02)) + 
  geom_histogram(bins = 30, color = 'black', fill = 'lightblue') + 
  scale_x_continuous(labels = scales::comma) + 
  ggtitle("  S02-Var02 Histogram Distribution")


ggplot(S02, aes(x = "", y = Var03)) + 
  geom_boxplot(fill = 'orange', color = 'black') + 
  ggtitle("S02-Var03 Box Plot")

ggplot(S02, aes(x = "", y = Var02)) + 
  geom_boxplot(fill = 'lightblue', color = 'black') + 
  ggtitle("S02-Var02 Box Plot")
```

Variable 03 looks normally distributed

Variable 02 is right skewed with potential outliers.

# Detect and replace outliers
```{r}

outliers_var2 <- tsoutliers(S02$Var02)
outliers_var2
S02$Var02[outliers_var2$index] <- outliers_var2$replacements


outliers_var3 <- tsoutliers(S02$Var03)
outliers_var3
S02$Var03[outliers_var3$index] <- outliers_var3$replacements


```

# Decomposition
```{r, warning=FALSE}

ts_var03 <- ts(S02$Var03, start = 2010, frequency = 365)
ts_var02 <- ts(S02$Var02, start = 2010, frequency = 365)
autoplot(decompose(ts_var03, type = "multiplicative")) + ggtitle("S02 Var03 Decomposition\n trend with clear seasonality")
autoplot(decompose(ts_var02, type = "multiplicative")) + ggtitle("S02 Var02 Decomposition\n decreasing trend with pontential seasonality")
```

# Differencing and Stationarity Checks
# KPSS Test /ACF Plot
```{r, warning=FALSE}
Var03_diff <- diff(S02$Var03)
S02$Var03_diff <- c(NA, Var03_diff)
ur.kpss(S02$Var03) %>% summary()
ur.kpss(S02$Var03_diff, type = "mu") %>% summary()

# KPSS Test and differencing for Var02
Var02_diff <- diff(S02$Var02)
S02$Var02_diff <- c(NA, Var02_diff)
ur.kpss(S02$Var02) %>% summary()
ur.kpss(S02$Var02_diff, type = "mu") %>% summary()

# ACF Plot for differenced values
ggtsdisplay(S02$Var03_diff, main="ACF Plot for Differenced Var03")
ggtsdisplay(S02$Var02_diff, main="ACF Plot for Differenced Var02")
```

The code checks if the data is smooth (stationary) or bumpy (non-stationary). This is because most forecasting models assume that the time series is stationary, we check to ensure accurate prediction.

For Var03 and Var02, initial test statistics (4.09, 11.30) indicate non-stationarity; after differencing (0.08, 0.004), data is stationary.

The ACF and PACF plots reconfirmed that. The quick drop-off to zero in the ACF plot confirms that there is no significant autocorrelation beyond the first few lags, which is a characteristic of stationary data. This implies that the data is suitable for applying forecasting models such as ARIMA and ETS.

 
#Seasonal Differencing check
```{r}

nsdiffs(ts(S02$Var03, frequency = 365))
nsdiffs(ts(S02$Var02, frequency = 365))
```
It's good practice to check for remaining seasonality using nsdiffs(), the 0 indicates NO seasonal patterns are left

# Model Building-ARIMA, ETS
```{r}
# Fit ARIMA model using auto.arima
# Splitting Data into Training & Validation Sets
break_num <- floor(nrow(S02) * 0.8)
train <- S02[1:break_num,]
val <- S02[(break_num + 1):nrow(S02),]


# ARIMA models
fit_var03_auto <-Arima(train$Var03, order=c(3,1,3), seasonal=c(2,1,2))

fit_var02_auto <- auto.arima(train$Var02, seasonal=TRUE)

# ETS models
fit_var03_ets <- ets(train$Var03)
fit_var02_ets <- ets(train$Var02)

summary(fit_var03_auto)
summary(fit_var03_ets)
summary(fit_var02_auto)
summary(fit_var02_ets)

```

# Forecasting on Validation Data & Select Best Model


```{r}
# Forecasting on Validation Data
forecast_var03_auto_val <- forecast(fit_var03_auto, h = nrow(val))
forecast_var03_ets_val <- forecast(fit_var03_ets, h = nrow(val))
forecast_var02_auto_val <- forecast(fit_var02_auto, h = nrow(val))
forecast_var02_ets_val <- forecast(fit_var02_ets, h = nrow(val))

# Function to calculate accuracy
calculate_accuracy <- function(forecast, actual) {
  accuracy(forecast, actual)
}

# Calculate accuracy for each forecast
accuracy_var03_auto_val <- calculate_accuracy(forecast_var03_auto_val, val$Var03)
accuracy_var03_ets_val <- calculate_accuracy(forecast_var03_ets_val, val$Var03)
accuracy_var02_auto_val <- calculate_accuracy(forecast_var02_auto_val, val$Var02)
accuracy_var02_ets_val <- calculate_accuracy(forecast_var02_ets_val, val$Var02)

# Combine results into a single data frame
accuracy_results <- bind_rows(
  data.frame(Model = "ARIMA_Var03", RMSE = accuracy_var03_auto_val["Test set", "RMSE"], MAPE = accuracy_var03_auto_val["Test set", "MAPE"]),
  data.frame(Model = "ETS_Var03", RMSE = accuracy_var03_ets_val["Test set", "RMSE"], MAPE = accuracy_var03_ets_val["Test set", "MAPE"]),
  data.frame(Model = "ARIMA_Var02", RMSE = accuracy_var02_auto_val["Test set", "RMSE"], MAPE = accuracy_var02_auto_val["Test set", "MAPE"]),
  data.frame(Model = "ETS_Var02", RMSE = accuracy_var02_ets_val["Test set", "RMSE"], MAPE = accuracy_var02_ets_val["Test set", "MAPE"])
)

# Display results
accuracy_results

# Residuals Check
checkresiduals(fit_var03_auto)
checkresiduals(fit_var02_auto)

```


RMSE (Root Mean Squared Error): Imagine throwing a ball at a target. RMSE tells you how far your throws are from the target, on average. Smaller numbers mean you are closer to the target more often.

MAPE (Mean Absolute Percentage Error): Think about how much you miss the target compared to how far you threw the ball. Smaller percentages mean your throws are more accurate.
Model Selection

Var03:

Choose ARIMA(3,1,3):

Better RMSE and MAPE on both training and validation sets.


Var02:

Choose ARIMA(1,1,1):

Better RMSE and MAPE on both training and validation sets.


The residuals (errors) for both selections are small and random, meaning the models fit well.


# Retrain the selected model with entire training set.
```{r}
# Refit the selected models on the entire dataset

fit_var03_final <- Arima(S02$Var03,order=c(3,1,3), seasonal=c(2,1,2))
fit_var02_final <- auto.arima(S02$Var02, seasonal=TRUE)
```


# Forecast140P

```{r}
# Forecast Var03
forecast_var03_auto <- forecast(fit_var03_final, h = 140)

# Forecast Var02
forecast_var02_auto <- forecast(fit_var02_final, h = 140)

# Plot forecasts
autoplot(forecast_var03_auto) + ggtitle("S02 Var03 \n ARIMA 140 periods Forecast")

autoplot(forecast_var02_auto) + ggtitle("S02 Var02 \n ARIMA 140 periods Forecast")

```

# Export Forecast

```{r}
# Filter the data for SeriesIND >= 43022 and sort it
prediction_label_S02 <- data %>% filter(SeriesInd >= 43022 & category == 'S02') %>%
  arrange(SeriesInd) %>%
  select(SeriesInd)

# Create a data frame with forecasted values for the next 140 periods for S02
forecast_data <- data.frame(
  SeriesInd = prediction_label_S02$SeriesInd,
  category = rep("S02", 140),
  Var02 = forecast_var02_auto$mean,
  Var03 = forecast_var03_auto$mean
)


#forecast_data
write.csv(forecast_data, "S02_140PeriodForecast.csv", row.names = FALSE)

```

