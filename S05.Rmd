---
title: "S05"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---



```{r, message=FALSE, warning=FALSE}
library(readxl)
library(dplyr)
library(imputeTS)
library(ggplot2)
library(forecast)
library(tseries)
library(gridExtra)
library(kableExtra)
library(scales)
library(openxlsx)
library(urca)
library(purrr)
```

# Load Data
```{r}
data <- read.csv("Data Set for Class.csv", stringsAsFactors = FALSE)
```

# Data Cleaning and Preprocessing
```{r}
S05 <- data %>% 
  mutate(date = as.Date(SeriesInd, origin = '1899-12-30')) %>%
  filter(SeriesInd < 43022 & category == 'S05') %>% 
  select(SeriesInd, date, category, Var03, Var02)

#S05
```
# Handle missing data
```{r}
S05$Var03 <- na_interpolation(S05$Var03)
S05$Var02 <- na_interpolation(S05$Var02)

glimpse(S05)
```
# Visualize Data

```{r}



ggplot(S05, aes(x = Var03)) + 
  geom_histogram(bins = 50, color = 'black', fill = 'orange') + 
  ggtitle("  S05-Var03 Histogram Distribution")

ggplot(S05, aes(x = Var02)) + 
  geom_histogram(bins = 30, color = 'black', fill = 'lightblue') + 
  scale_x_continuous(labels = scales::comma) + 
  ggtitle("  S05-Var02 Histogram Distribution")


ggplot(S05, aes(x = "", y = Var03)) + 
  geom_boxplot(fill = 'orange', color = 'black') + 
  ggtitle("S05-Var03 Box Plot")

ggplot(S05, aes(x = "", y = Var02)) + 
  geom_boxplot(fill = 'lightblue', color = 'black') + 
  ggtitle("S05-Var02 Box Plot")
```

Var03: Nearly normally distributed.

Var02: Right skewed with most values in low end and some very high, indicating potential outliers.

# Detect and replace outliers
```{r}

outliers_var2 <- tsoutliers(S05$Var02)
outliers_var2
S05$Var02[outliers_var2$index] <- outliers_var2$replacements


outliers_var3 <- tsoutliers(S05$Var03)
outliers_var3
S05$Var03[outliers_var3$index] <- outliers_var3$replacements


```

# Decomposition
```{r, warning=FALSE}

ts_var1 <- ts(S05$Var03, start = 2010, frequency = 365.25)
ts_var2 <- ts(S05$Var02, start = 2010, frequency = 365.25)
autoplot(decompose(ts_var1, type = "multiplicative")) + ggtitle("S05 Var03 Decomposition\n trend with clear seasonality")
autoplot(decompose(ts_var2, type = "multiplicative")) + ggtitle("S05 Var02 Decomposition\n decreasing trend with pontential seasonality")
```

# Differencing and Stationarity Checks
# KPSS Test /ACF Plot
```{r, warning=FALSE}
Var03_diff <- diff(S05$Var03)
S05$Var03_diff <- c(NA, Var03_diff)
ur.kpss(S05$Var03) %>% summary()
ur.kpss(S05$Var03_diff, type = "mu") %>% summary()

# KPSS Test and differencing for Var02
Var02_diff <- diff(S05$Var02)
S05$Var02_diff <- c(NA, Var02_diff)
ur.kpss(S05$Var02) %>% summary()
ur.kpss(S05$Var02_diff, type = "mu") %>% summary()

# ACF Plot for differenced values
ggtsdisplay(S05$Var03_diff, main="ACF Plot for Differenced Var03")
ggtsdisplay(S05$Var02_diff, main="ACF Plot for Differenced Var02")
```

The code checks if the data is smooth (stationary) or bumpy (non-stationary). This is because most forecasting models assume that the time series is stationary, we check to ensure accurate prediction.

For Var03 and Var02, initial test statistics (8.0595, 9.061) indicate non-stationarity; after differencing (0.0709, 0.0062), data is stationary.

The ACF and PACF plots reconfirmed data stable after differencing, lag patterns look fine. 

 
# Seasonal Differencing check
```{r}

nsdiffs(ts(S05$Var03, frequency = 365))
nsdiffs(ts(S05$Var02, frequency = 365))
```
It's good practice to check for remaining seasonality using nsdiffs(), the 0 indicates NO seasonal differencing required. 

# Model Building-ARIMA, ETS
```{r}
# Fit ARIMA model using auto.arima
# Splitting Data into Training & Validation Sets
break_num <- floor(nrow(S05) * 0.8)
train <- S05[1:break_num,]
val <- S05[(break_num + 1):nrow(S05),]


# ARIMA models
fit_var03_auto <-Arima(train$Var03, order=c(3,1,3), seasonal=c(2,1,2))

fit_var02_auto <- auto.arima(train$Var02, seasonal=TRUE)

# ETS models
fit_var03_ets <- ets(train$Var03)
fit_var02_ets <- ets(train$Var02)

summary(fit_var03_auto)
summary(fit_var03_ets)
summary(fit_var02_auto)
summary(fit_var02_ets)

```

# Forecasting on Validation Data & Select Best Model


```{r}
# Forecasting on Validation Data
forecast_var03_auto_val <- forecast(fit_var03_auto, h = nrow(val))
forecast_var03_ets_val <- forecast(fit_var03_ets, h = nrow(val))
forecast_var02_auto_val <- forecast(fit_var02_auto, h = nrow(val))
forecast_var02_ets_val <- forecast(fit_var02_ets, h = nrow(val))

# Function to calculate accuracy
calculate_accuracy <- function(forecast, actual) {
  accuracy(forecast, actual)
}

# Calculate accuracy for each forecast
accuracy_var03_auto_val <- calculate_accuracy(forecast_var03_auto_val, val$Var03)
accuracy_var03_ets_val <- calculate_accuracy(forecast_var03_ets_val, val$Var03)
accuracy_var02_auto_val <- calculate_accuracy(forecast_var02_auto_val, val$Var02)
accuracy_var02_ets_val <- calculate_accuracy(forecast_var02_ets_val, val$Var02)

# Combine results into a single data frame
accuracy_results <- bind_rows(
  data.frame(Model = "ARIMA_Var03", RMSE = accuracy_var03_auto_val["Test set", "RMSE"], MAPE = accuracy_var03_auto_val["Test set", "MAPE"]),
  data.frame(Model = "ETS_Var03", RMSE = accuracy_var03_ets_val["Test set", "RMSE"], MAPE = accuracy_var03_ets_val["Test set", "MAPE"]),
  data.frame(Model = "ARIMA_Var02", RMSE = accuracy_var02_auto_val["Test set", "RMSE"], MAPE = accuracy_var02_auto_val["Test set", "MAPE"]),
  data.frame(Model = "ETS_Var02", RMSE = accuracy_var02_ets_val["Test set", "RMSE"], MAPE = accuracy_var02_ets_val["Test set", "MAPE"])
)

# Display results
accuracy_results

# Residuals Check
checkresiduals(fit_var03_auto)
checkresiduals(fit_var03_ets)
checkresiduals(fit_var02_auto)
checkresiduals(fit_var02_ets)



```


RMSE (Root Mean Squared Error): Imagine throwing a ball at a target. RMSE tells you how far your throws are from the target, on average. Smaller numbers mean you are closer to the target more often.

MAPE (Mean Absolute Percentage Error): Think about how much you miss the target compared to how far you threw the ball. Smaller percentages mean your throws are more accurate.

Model Selection


Var03:
Choose ARIMA(3,1,3):

Good RMSE and MAPE on both training and validation sets.
Residuals (errors) are small and random, meaning the model fits well.
ETS: Although slightly better performance on validation, residuals fail the Ljung-Box test, indicating significant autocorrelation (errors not random).


Var02:
Choose ARIMA(1,1,2):

Good RMSE and MAPE on both training and validation sets.
Residuals (errors) are small and random, meaning the model fits well.
ETS: Despite better MAPE, residuals fail the Ljung-Box test, indicating significant autocorrelation (errors not random).

# Retrain the selected model with entire training set.
```{r}
# Refit the selected models on the entire dataset

fit_var03_final <- Arima(S05$Var03,order=c(3,1,3), seasonal=c(2,1,2))
fit_var02_final <- auto.arima(S05$Var02, seasonal=TRUE)
```


# Forecast140P

```{r}
# Forecast Var03
forecast_var03_auto <- forecast(fit_var03_final, h = 140)

# Forecast Var02
forecast_var02_auto <- forecast(fit_var02_final, h = 140)

# Plot forecasts
autoplot(forecast_var03_auto) + ggtitle("S05 Var03 \n ARIMA 140 periods Forecast")

autoplot(forecast_var02_auto) + ggtitle("S05 Var02 \n ARIMA 140 periods Forecast")

```

# Export Forecast

```{r}
# Filter the data for SeriesIND >= 43022 and sort it
prediction_label_S05 <- data %>% filter(SeriesInd >= 43022 & category == 'S05') %>%
  arrange(SeriesInd) %>%
  select(SeriesInd)

# Create a data frame with forecasted values for the next 140 periods for S05
forecast_data <- data.frame(
  SeriesInd = prediction_label_S05$SeriesInd,
  category = rep("S05", 140),
  Var02 = forecast_var02_auto$mean,
  Var03 = forecast_var03_auto$mean
)


forecast_data
write.csv(forecast_data, "S05_140PeriodForecast.csv", row.names = FALSE)

```

